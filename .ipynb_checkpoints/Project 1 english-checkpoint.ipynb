{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the subject and the mathematical notions involved\n",
    "### 1) Context and motivation\n",
    "\n",
    "**Regression** is used to find a relationship between a variable $y$ and one or more other variables $x_j$.\n",
    "\n",
    "In machine learning, the goal of regression is to estimate a numerical output value $y$, called _target_, from the values ​​of a set of input features $x_j$, called _explanatory variables_ . For example, estimate the price of a house ($y$) based on its surface area ($x_1$), the number of floors ($x_2$), its location ($x_3$), its orientation ($ x_4$) etc\n",
    "\n",
    "Thus, the problem amounts to estimating a computational function $\\hat{y}(x)$ based on training data $x_j \\ (\\forall j \\in \\{1,p\\})$:\n",
    "\n",
    "$$ \\hat{y}(x) = f(x_1, x_2, \\dots, x_p)$$\n",
    "\n",
    "The estimation of this function allows:\n",
    "* to measure the explanatory power of the model\n",
    "* to evaluate the influence of the variables $x_j$ in the model\n",
    "* to evaluate the quality of the model during the prediction (prediction interval)\n",
    "* to detect observations that may overly influence the results (atypical points).\n",
    "\n",
    "There are several algorithms for regression; the one we are interested in here is **linear regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Single VS multiple linear regression\n",
    "\n",
    "**Simple** linear regression is used to find a relationship between one output variable with respect to just one other:\n",
    "\n",
    "$$ \\hat{y}(x) = a_0 + a_1 x_1$$\n",
    "\n",
    "where $a_0$ and $a_1$ are called the coefficients of the regression line.\n",
    "\n",
    "But in practice, it is very rare to find a variable $y$ which only depends on a single other variable $x_1$. It then becomes necessary to create models linking a dependent variable $y$ which fluctuates according to several independent explanatory variables $x_j$.\n",
    "A model with more than one independent variable is called a **multiple** linear regression model. It is used to estimate a linear function between the output and the $p$ independent variables:\n",
    "\n",
    "$$ \\hat{y}(x) = a_0 + a_1 x_1 + a_2 x_2 + \\dots + a_p x_p $$\n",
    "or :\n",
    "* $\\hat{y}$ is the estimated output (target prediction)\n",
    "* $x_j \\ (j\\in \\{1,p\\})$ are the independent explanatory variables\n",
    "* $a_j \\ (j\\in \\{0,p\\})$ are the regression coefficients\n",
    "\n",
    "$\\longrightarrow$ the purpose of the regression method is to estimate these coefficients $a_j$ in order to predict the output variable $\\hat{y}$ based on input data.\n",
    "\n",
    "#### Matrix writing of the multiple linear regression model\n",
    "When we have a dataset of $n$ observations $(y_i, x_{i1}, \\dots, x_{ip}), i \\in \\{1, n\\}$, the equation of multiple linear regression is written:\n",
    "\n",
    "$$ \\underbrace{y_i}_{\\text{target}} = \\underbrace{a_0 + a_1 x_{i1} + a_2 x_{i2} + \\dots + a_p x_{ip}}_{\\text{estimate} \\ \\hat{y} \\ \\text{search}} + \\underbrace{\\varepsilon_i}_{\\text{error}}, \\quad i \\in \\{1,n\\}$$\n",
    "\n",
    "\n",
    "$$ \\Longleftrightarrow {\\displaystyle {\\begin{cases}y_{1}=a_{0}+a_{1}x_{1,1}+\\ldots +a_{p}x_{1,p}+\\varepsilon _{1}\\\\y_{2}=a_{0}+a_{1}x_{2,1}+\\ldots +a_{p}x_{2,p}+\\varepsilon _{2}\\\\\\ cdots \\\\y_{n}=a_{0}+a_{1}x_{n,1}+\\ldots +a_{p}x_{n,p}+\\varepsilon _{n}\\end{cases}} } $$\n",
    "\n",
    "Which can be summarized with the following matrix notation:\n",
    "\n",
    "$${\\displaystyle {\\begin{pmatrix}y_{1}\\\\\\vdots \\\\y_{n}\\end{pmatrix}}={\\begin{pmatrix}1&x_{1,1}&\\cdots &x_{1 ,p}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\1&x_{n,1}&\\cdots &x_{n,p}\\end{pmatrix}} \\ {\\begin{pmatrix}a_{0} \\\\a_{1}\\\\\\vdots \\\\a_{p}\\\\\\end{pmatrix}}+{\\begin{pmatrix}\\varepsilon _{1}\\\\\\vdots \\\\\\varepsilon _{n}\\ \\\\end{pmatrix}}} $$\n",
    "\n",
    "Or in a compact way:\n",
    "\n",
    "$$y=Xa+\\varepsilon $$\n",
    "or\n",
    "* $y$ is a vector of dimension $(n, 1)$\n",
    "* $X$ is a matrix of dimension $(n, p+1)$\n",
    "* $a$ is a vector of dimension $(p+1, 1)$\n",
    "* $\\varepsilon$ is a vector of dimension $(n, 1)$\n",
    "\n",
    "Important note: The first column of the matrix $X$ is used to indicate that the regression is carried out with a constant (here $a_0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Estimation of the regression coefficients $a_j$ using the least squares approach\n",
    "\n",
    "From the complete model, we seek the coefficients $a_j$ ($j \\in \\{0,p\\}$) to obtain:\n",
    "\n",
    "$$ \\hat{y}_i = a_0 + a_1 x_{i1} + a_2 x_{i2} + \\dots + a_p x_{ip} $$\n",
    "\n",
    "The estimated residuals $\\varepsilon$ are the difference between the observed value $y$ and the estimated value $\\hat{y}$. That is :\n",
    "\n",
    "$$ \\varepsilon_i = y_i - \\hat{y}_i$$\n",
    "\n",
    "The least squares approach consists in finding the values ​​of the coefficients $a_j$ which minimize the sum of the squares of the residuals:\n",
    "\n",
    "$$S = \\dfrac{1}{2n} \\sum _{i=1}^{n}{\\varepsilon }_{i}^{2}$$\n",
    "\n",
    "In other words, we are looking for:\n",
    "\n",
    "$${\\displaystyle \\min \\frac{1}{2n}\\sum _{i=1}^{n}{\\varepsilon }_{i}^{2}=\\min _{a_{0},. ,a_{p}}\\frac{1}{2n}\\sum _{i=1}^{n}(y_{i}-{a}_{0}-{a}_{1}x_{i ,1}-\\cdots -{a}_{p}x_{i,p})^{2}} $$\n",
    "\n",
    "The $S$ function is a convex function, which means that it has a unique global minimum. To find it, you have to cancel the gradient of $S$, i.e. look for the solutions of:\n",
    "\n",
    "$$\\nabla S = 0$$\n",
    "\n",
    "$$\\Longleftrightarrow {\\displaystyle { \\frac{\\partial S}{\\partial a_j} = 0 \\Longleftrightarrow \\frac {\\partial (\\frac{1}{2n}\\sum {\\varepsilon }_{i}^{ 2})}{\\partial {a}_{j}}}=0\\, \\qquad \\forall j \\in \\{0,p\\}} $$\n",
    "\n",
    "So there are $p + 1$ equations to solve. Two methods can be considered for this:\n",
    "* a direct method of system inversion\n",
    "* a gradient descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3)a) Direct method\n",
    "\n",
    "We want to solve the $p+1$ equations:\n",
    "\n",
    "$${\\displaystyle {\\frac{\\partial S}{\\partial a_j} = \\frac {\\partial (\\frac{1}{2n}\\sum {\\varepsilon }_{i}^{2})}{ \\partial {a}_{j}}}=0\\,} $$\n",
    "\n",
    "By passing the derivation operator in the sum, we have that each partial derivative $\\forall j \\in \\{0,p\\}$ is equal to:\n",
    "\n",
    "$$\\frac{\\partial S}{\\partial a_j}=\\frac{1}{2n} \\sum _{i=1}^{n}(-2x_{i,j})(y_{i}- {a}_{0}-{a}_{1}x_{i,1}-\\cdots -{a}_{p}x_{i,p}) \\qquad (1)$$\n",
    "\n",
    "Thereby :\n",
    "$$\\frac{\\partial S}{\\partial a_j} =0$$\n",
    "$$\\Longleftrightarrow {\\displaystyle \\sum _{i=1}^{n}x_{i,j}(y_{i}-{a}_{0}-{a}_{1}x_{i, 1}-\\cdots -{a}_{p}x_{i,p})=0} $$\n",
    "\n",
    "By writing this last relation in vector form, we then obtain the following linear system:\n",
    "\n",
    "$$X^{T}(Y-Xa)=0$$\n",
    "$$\\Longleftrightarrow (X^TX)a=X^TY$$\n",
    "The solution of which is:\n",
    "$$\\boxed{a=(X^TX)^{-1}X^TY} \\qquad (2)$$\n",
    "\n",
    "$\\Longrightarrow$ The resolution of this linear system can then be done by a direct method like the method of the pivot of **Gauss** or the method of **decompositon LU** ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3)b) Fixed step gradient descent method\n",
    "\n",
    "The minimization problem we are interested in here (ie $\\nabla S = 0$) can be solved using a gradient descent which consists in constructing, within an iterative algorithm, the iterated vectors $ a^k \\equiv (a_0, a_1, \\dots, a_p)^k$ as follows:\n",
    "\n",
    "$$\\boxed{a^{k+1} = a^k - \\alpha (\\nabla S)^k} \\qquad (3)$$\n",
    "\n",
    "where $\\alpha$ is the fixed descent step and where, according to the relation $(1)$, the gradient vector $\\nabla S$ at the iteration $k$ is given by:\n",
    "\n",
    "$$\\nabla S^k = \\begin{pmatrix}\\frac{\\partial S^k}{\\partial a_0} = \\frac{1}{n} \\sum _{i=1}^{n}(- x_{i,0})(y_{i}-\\langle a^k,x_i \\rangle ) \\\\ \\frac{\\partial S^k}{\\partial a_1} = \\frac{1}{n} \\sum _{i=1}^{n}(-x_{i,1})(y_{i}-\\langle a^k,x_i \\rangle ) \\\\\\vdots \\\\ \\frac{\\partial S^k} {\\partial a_p} = \\frac{1}{n} \\sum _{i=1}^{n}(-x_{i,p})(y_{i}-\\langle a^k,x_i \\rangle )\\\\\\end{pmatrix} \\qquad (4)$$\n",
    "\n",
    "The convergence of the vector $a^k$ towards the vector of regression coefficients sought is reached when $\\nabla S^k < \\varepsilon$, where $\\varepsilon$ is the desired precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "## Questions\n",
    "\n",
    "### Part A: Algorithms and numerical methods\n",
    "\n",
    "First start by executing the cell below to import the different packages useful for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.datasets import make_regression\n",
    "from scipy.stats import spearmanr, pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Import into the cell below the `bsub`, `pivotgauss` and `gauss` functions. (Look at TP 2 and TP 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste in this same cell your 3 functions: \"bsub\", \"pivotgauss\" and \"gauss\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Take the `GradientNotFixed` function from TP9 and modify it so that it solves the problem defined by the relations $(3)$ and $(4)$ above.\n",
    "\n",
    "<details>\n",
    "    <summary> ${Index 1}$ </summary>\n",
    "    <p> Concerning the input arguments of the function, we will keep: <br>\n",
    "        - \"alpha\" (the fixed descent pitch) <br>\n",
    "        - \"epsilon\" (the desired precision, possibly as an optional argument) <br>\n",
    "        - \"itmax\" (the max number of iterations, possibly as an optional argument).<br>\n",
    "     Instead of the matrix $A$ and the vector $b$ (cf course of TP 9) we will give as input the data of the regression problem, namely the target vector and the matrix of the explanatory variables. <br><br>\n",
    "       Concerning the outputs of the function there will be: <br>\n",
    "        - the solution of the problem <br>\n",
    "        - \"it\" (the number of iterations that were needed to converge) <br>\n",
    "        - \"norm\" (the norm of $\\nabla S$).<br>\n",
    "    </p>\n",
    "    \n",
    "</details><br>\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary> ${Subscript 2}$ </summary>\n",
    "    <p> As we do not know here the equivalent of the matrix $A$ and the vector $b$, we cannot calculate the residual vector $ r^k = b- Ax^k$. You must therefore modify your gradient descent algorithm by reasoning directly from the gradient of $S$ (ie $\\nabla S$), both for the calculation of iterates $a^{k+1} = a^k - \\alpha (\\nabla S)^k$ only for the stopping criterion of the algorithm.</p>\n",
    "    \n",
    "</details><br>\n",
    "\n",
    "<details>\n",
    "    <summary> ${Index 3}$ </summary>\n",
    "    <p> To save execution time, avoid \"for\" loops as much as possible when implementing the formulas of the relation $(4)$ above. Think of operations as dot products (np.dot).</p>\n",
    "    \n",
    "</details><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientPasFixe(?):\n",
    "    \n",
    "    return ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Validation of the Modified Fixed-Step Gradient Descent Algorithm\n",
    "\n",
    "The purpose of this part B is to validate your `GradientNotFixed` function implemented above on the following small dataset, consisting of $n=10$ observations and $p=3$ explanatory variables (note that the first column $ x_0$ is indeed the unit vector, as formulated in the first part of the subject):\n",
    "\n",
    "| $x_0$ | $x_1$ | $x_2$ | $y$ |\n",
    "| ------------- |: -------------: | ---------: | ---------: |\n",
    "| 1 | 0.72 | 0.32 | 6.93 |\n",
    "| 1 | 0.75 | 0.12 | 5.99 |\n",
    "| 1 | 0.53 | 0.65 | 1.46 |\n",
    "| 1 | 0.27 | 0.82 | 1.44 |\n",
    "| 1 | 0.49 | 0.15 | 4.51 |\n",
    "| 1 | 0.02 | 0.19 | 1.25 |\n",
    "| 1 | 0.35 | 0.87 | 2.53 |\n",
    "| 1 | 0.99 | 0.71 | 6.88 |\n",
    "| 1 | 0.98 | 0.92 | 6.25 |\n",
    "| 1 | 0.73 | 0.19 | 6.36 |\n",
    "\n",
    "To extract this dataset, import the `test.dat` data file into your Jupyter directory and then run the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.loadtxt('test.dat', delimiter=' ')\n",
    "print(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** From the `data_test` variable above, extract in a `X` variable the matrix of the explanatory variables $x_0, x_1, x_2$ and in a `Y` variable the target vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Apply your function `GradientNotFixed` to this dataset by taking a descent step `alpha` $=1.05$ and check that you obtain the following regression coefficients at the output of the algorithm:\n",
    "\n",
    "$$ a = (a_0, a_1, a_2) = (1.424, \\ 7.173, -2.523)$$\n",
    "\n",
    "To help you validate (or debug) your gradient descent algorithm, here are the results that your algorithm should return to you for the first iterations:\n",
    "\n",
    "\n",
    "|$k$  |$a_0$ |$a_1$ |$a_2$ | $\\frac{dS}{d a_0}$ | $\\frac{dS}{d a_1}$ | $\\frac{dS}{d a_2}$ |\n",
    "| --- |: --- |: --- |: --- |: ------ |: ------ |: ------ |\n",
    "|0    |0.100 |0.100 |0.100 |-4.152   |-3.003   |-1.889   |\n",
    "|1    |4.460 |3.253 |2.083 |3.026    |1.483    |1.892    |\n",
    "|2    |1.283 |1.697 |0.097 |-2.040   |-1.633   |-0.825   |\n",
    "|3    |3.425 |3.411 |0.963 |1.529    |0.609    |1.045    |\n",
    "|4    |1.819 |2.771 |-0.135|-0.992   |-0.931   |-0.315   |\n",
    "|5    |2.860 |3.749 |0.196 |0.783    |0.193    |0.606    |\n",
    "|6    |2.039 |3.546 |-0.440|-0.472   |-0.564   |-0.078   |\n",
    "|7    |2.534 |4.138 |-0.358|0.410    |0.002    |0.373    |\n",
    "|8    |2.104 |4.135 |-0.750|-0.216   |-0.367   |0.026    |\n",
    "|9    |2.330 |4.521 |-0.778|0.222    |-0.079   |0.245    |\n",
    "|...  |...   |...   |...   |...      |...      |...      |\n",
    "|...  |...   |...   |...   |...      |...      |...      |\n",
    "|30   |1.658 |6.618 |-2.314|0.013    |-0.038   |0.019    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)a)** Plot the cloud of points representing $Y$ (target) as a function of $\\hat{Y} = X\\cdot a$ (prediction). If the estimate of the coefficients $a$ is good, the scatter plot must \"align\" along the bisector, which will also be drawn on the same graph.\n",
    "\n",
    "<details>\n",
    "     <summary> ${Hint }$ </summary>\n",
    "     <p> To plot the bisector we can simply enter the command: plt.plot(Y,Y)</p>\n",
    "    \n",
    "</details><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)b)** We can also measure the linear correlation between $Y$ and $\\hat{Y}$ using the `pearsonr` function (see [documentation](https://docs.scipy.org/ doc/scipy/reference/generated/scipy.stats.pearsonr.html)) from the `scipy.stats` library (already imported at the beginning of the notebook). This function returns as its first output value the Pearson regression coefficient. This coefficient is between 0 and 1 in absolute value. The closer it is to 1, the more the variables $Y$ and $\\hat{Y}$ have a linear relationship (for a Pearson coefficient equal to 1 the linear relationship is exact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the `pearsonr` function to know the linear relation between Y and ^Y:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Comparison of the direct method and the gradient descent method on generated datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part C is to compare your Gaussian method (direct method) with your gradient descent method (iterative method) on automatically and randomly generated datasets using the `make_regression` function of the `sklearn library. datasets` (imported at the beginning of notebook).\n",
    "\n",
    "**1) Noisy dataset with $n = 200$ observations and $p = 7$ explanatory variables**\n",
    "\n",
    "Execute the cell below in order to load respectively in the variables `X` and `Y` the matrix of explanatory variables and the target vector, generated randomly. The last two lines of code add a unit vector in the first position of the $X$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the given set\n",
    "X, Y = make_regression(n_samples=200, n_features=7, n_targets=1, noise = 50, n_informative=7)\n",
    "\n",
    "# adding a unit vector to the first column of X\n",
    "V_unit = np.ones((X.shape[0], 1))\n",
    "X = np.hstack((V_unit, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> WARNING: If you re-execute this cell above a second time, the game will be generated again and will be different from the previous one. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the generated dataset, perform all of the following steps:\n",
    "\n",
    "1. **Resolution with the method of Gauss**:\n",
    "    * find the regression coefficients $a$:\n",
    "    $$a = (X^TX)^{-1}X^TY$$\n",
    "    using Gaussian method. We will use the `%time` command at the beginning of the `gauss` function call line to measure the execution time.\n",
    "    * plot the cloud of points $Y$ as a function of $\\hat{Y}$ and the bisector on the same graph.\n",
    "    * give the Pearson correlation value\n",
    "2. **Resolution with the gradient descent method**:\n",
    "    * For 50 values ​​of `alpha` taken in an interval close to $[0.2, 1.3]$ (or any other interval you think is more appropriate), call your `GradientNotFixed` function and store the 50 outputs in arrays.\n",
    "    * plot the number of iterations that were necessary as a function of `alpha` as well as the norm of $\\nabla S$ at the end of the algorithm as a function of `alpha`. We can use this [tutorial (footer)](https://cmdlinetips.com/2019/10/how-to-make-a-plot-with-two-different-y-axis-in-python-with -matplotlib/) or [this one](https://stackoverflow.com/questions/5484922/secondary-axis-with-twinx-how-to-add-to-legend) to plot the 2 curves on the same graph but with different scales in ordinates for the iterations and for the norm.\n",
    "    * based on the previous result, determine the best value of `alpha` and call your `GradientNotFixed` function with this optimal value. We will use the `%time` command at the beginning of the call line of the `GradientNotFixed` function to measure the execution time.\n",
    "    * plot the cloud of points $Y$ as a function of $\\hat{Y}$ and the bisector on the same graph.\n",
    "    * give the Pearson correlation value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolution for method 1: direct method (Gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolution for method 2: iterative method (gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Noisy dataset with $n = 200$ observations and $p = 100$ explanatory variables**\n",
    "\n",
    "Run the cell below to generate a new dataset with as many observations but now having 100 explanatory variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> WARNING: By executing this cell, you overwrite the previous content of the 'X' and 'Y' variables (this is not a problem in itself, but you can rename the variables 'X' and 'Y' if you prefer). Likewise, if you re-execute this cell below a second time, the game will be generated again and will be different from the previous one. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_regression(n_samples=200, n_features=100, n_targets=1, noise = 5000, n_informative=100)\n",
    "V_unit = np.ones((X.shape[0], 1))\n",
    "X = np.hstack((V_unit, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat here all the steps performed on the previous dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolution for method 1: direct method (Gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolution for method 2: iterative method (gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Analysis and conclusion:**\n",
    "\n",
    "Based on your results obtained on the two sets of data above, write a concluding paragraph based on the following questions:\n",
    "* Do the two methods give the same result in terms of regression?\n",
    "* Are the two methods equivalent in terms of computation time (see result of the `%time` command)? Which is the fastest? Can you explain why?\n",
    "* Depending on the size of the data set (number of observations $(n)$ versus number of explanatory variables $(p)$) which method do you think is the most appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Application to a real dataset and machine learning\n",
    "\n",
    "The objective of this last part is to apply the direct method and the gradient descent method on a real dataset.\n",
    "This dataset provides the diabetes measurements of $n=432$ patients (which correspond here to the number of observations). More specifically, it provides the measurement of diabetes progression for these patients (target), based on the following explanatory variables $p=10$:\n",
    "\n",
    "| Title of $x_j$ | Description |\n",
    "| -------------------- |: ------------------: |\n",
    "| `AGE` | age in years |\n",
    "| `SEX` | sex |\n",
    "| `BMI` | body mass index |\n",
    "| `BP` | average blood pressure |\n",
    "| `S1` | total serum cholesterol |\n",
    "| `S2` | low density lipoprotein (LDL)|\n",
    "| `S3` | high density lipoprotein (HDL)|\n",
    "| `S4` | total cholesterol / HDL |\n",
    "| `S5` | triglyceride level |\n",
    "| `S6` | blood sugar level |\n",
    "\n",
    "Here is the original dataset:\n",
    "https://hastie.su.domains/Papers/LARS/diabetes.data\n",
    "\n",
    "Before processing, this data set is normalized in such a way as to impose a zero mean and a Euclidean norm equal to 1 for all the independent variables $x_j$ and a zero mean for the last column $y$ (target). It is this **normalized dataset** that will be used here. To extract it, import into Jupyter the `diabetes.csv` data file published on Moodle then execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabete = pd.read_csv('diabete.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to observe the extracted dataset, run the following two cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this cell to load the data into the `Xdiab` (explanatory variables $x_j$) and `Ydiab` (target) variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabete = df_diabete.to_numpy()\n",
    "Xdiab = diabete[:,:-1]\n",
    "Ydiab = diabete[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed that within each column, the data undergoes significant variations according to the patients. In order to make the regression calculations more efficient, it is necessary to ensure that the data are expressed on the same scale; this is called _scaling_. This scaling is done by redefining the data as follows:\n",
    "\n",
    "$$ x_{i,j} = \\dfrac{x_{i,j} - \\mu_{xj}}{r_{x_j}}, \\qquad \\forall \\ i \\in \\{1,n\\}, \\quad \\forall \\ j \\in \\{1,p\\}$$\n",
    "\n",
    "$$ y_i = \\dfrac{y_i - \\mu_{y}}{r_{y}}, \\qquad \\forall \\ i \\in \\{1,n\\}$$\n",
    "\n",
    "with: $\\mu_{xj} = {\\text{average}}(x_j) \\quad $ and $ \\quad r_{x_j} = \\max(x_j) - \\min(x_j), \\qquad \\forall \\ j \\ in \\{1,p\\}$\n",
    "with: $\\mu_{y} = {\\text{average}}(y)\\quad $ and $ \\quad r_{yi} = \\max(y) - \\min(y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Perform this scaling on the `Xdiab` matrix and on the `Ydiab` vector. Once the scaling is done, add a unit vector in the first column of the `Xdiab` matrix (see part C where this operation has already been performed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Compare the direct method (Gauss) with the gradient descent method by repeating the same steps as those carried out in part C on the automatically generated data sets.\n",
    "\n",
    "<details>\n",
    "     <summary> ${Hint }$ </summary>\n",
    "     <p> For the gradient descent method, don't hesitate to play with the \"epsilon\" precision parameter and with the \"alpha\" search interval to find the best descent step. </p>\n",
    "    \n",
    "</details><br>\n",
    "\n",
    "Make sure your two methods give similar results. Also observe the calculation times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolution for method 1: direct method (Gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolution for method 2: iterative method (gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** For a **learning** purpose, we now want to use the result of the regression to evaluate the influence of the variables $x_j$ on diabetes.\n",
    "\n",
    "For this, we propose to draw a horizontal bar chart (_horizontal bar chart_) representing each explanatory variable $x_j$ with a horizontal bar whose length is proportional to the associated regression coefficient.\n",
    "\n",
    "**a)** To plot this graph, run the cell below replacing the `?` with the list of regression coefficients $a_j$ associated with the variables `age`, `sex`, ...,`s6 ` i.e. at $x_1, x_2, \\dots, x_{10}$ (be careful, no $a_0$ in this graph). You can use the regression coefficients from the method of your choice (direct method or gradient descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_feature_names = ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "\n",
    "coefs = pd.DataFrame(\n",
    "    ?, # replace here the \"?\" by the table containing the regression coefficients\n",
    "    columns=[\"Coefficients\"],\n",
    "    index=diabetes_feature_names,\n",
    ")\n",
    "\n",
    "coefs.plot(kind=\"barh\", figsize=(9, 7))\n",
    "plt.title(\"Importance of the explanatory variables with respect to the regression coefficients\")\n",
    "plt.axvline(x=0, color=\".5\")\n",
    "plt.subplots_adjust(left=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** From the graph above, what are the 3 most important risk factors in the production of diabetes in these 432 patients?\n",
    "${Hint }$ </summary>\n",
    "In a horizontal histogram, a negative bar means an increase in the associated variable implies a reduction in diabetes, and a positive bar means an increase in the associated variable implies an increase in diabetes. </p>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** For the purpose of **prediction** this time, we now want to use the result of the regression to evaluate the measure of diabetes in 10 new patients whose medical data are as follows:\n",
    "\n",
    "_Import into Jupyter the `diabetes_nouveaux_patients.csv` data file published on Moodle then execute the cell below to load the patient data into the `Xnouv` variable (the `Ynouv` variable contains the actual diabetes measurement for each patient and will be used for validation in question 4)c) below)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabete_nouv = pd.read_csv('diabete_nouveaux_patients.csv', index_col = 0)\n",
    "diabete_nouv = df_diabete_nouv.to_numpy()\n",
    "diabete_nouv\n",
    "\n",
    "Xnouv = diabete_nouv[:,:-1]\n",
    "Ynouv = diabete_nouv[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** As for question **D)1)**, perform the scaling on the `Xnew` matrix and the `Ynew` vector. Once the scaling is done, add a unit vector in the first column of the `Xnouv` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Estimate the measures of diabetes associated with these 10 new patients using the regression results of the method of your choice (Gauss or gradient descent), and place these predictive measures $\\hat{Y }_{new}$ in a `Ynew_pred` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Plot the scatter plot $Y$ again as a function of $\\hat{Y}$ from the initial dataset (`Xdiab` and `Ydiab`) and add to it (with another color ) the scatter plot $Y_{new}$ according to $\\hat{Y}_{new}$ from the data of the new patients (`Xnew` and `Ynew`). Check that the new points are well integrated into the original cloud and are also distributed along the bisector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
